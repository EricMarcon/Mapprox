---
title: "Traitement des données de grande taille avec M"
author:
  - name: "Eric Marcon"
  - name: "Florence Puech"
abstract: >
  Ce document teste l'impact de l'approximation de la position des points sur la précision de $M$ et le temps de calcul.
  Dans un deuxième temps, les besoins en temps et en mémoire du calcul exact avec le package dbmss sont évalués.
date: "`r format(Sys.time(), '%d %B %Y')`"
url: https://EricMarcon.github.io/Mapprox/
github-repo: EricMarcon/Mapprox
# Language
lang: fr-FR
# Bibliography
bibliography: references.bib
biblio-style: chicago
# LaTeX
# Print table of contents in PDFs?
pdftoc: false
# If true, choose its depth
toc-depth: 3
# URL color
urlcolor: blue
# Do not modify
always_allow_html: yes
csquotes: true
output:
  rmdformats::downcute:
    use_bookdown: yes
    lightbox: yes
    pandoc_args: "--lua-filter=fr-nbsp.lua"
  bookdown::pdf_book:
    template: latex/template.tex
    citation_package: natbib
    latex_engine: xelatex
    keep_tex: yes
---

```{r DoNotModify, include=FALSE}
### Utilities. Do not modify.
# Installation of packages if necessary
InstallPackages <- function(Packages) {
  InstallPackage <- function(Package) {
    if (!Package %in% installed.packages()[, 1]) {
      install.packages(Package, repos="https://cran.rstudio.com/")
    }
  }
  invisible(sapply(Packages, InstallPackage))
}

# Basic packages
InstallPackages(c("bookdown", "formatR", "ragg"))

# Chunk font size hook: allows size='small' or any valid Latex font size in chunk options
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
```

```{r Options, include=FALSE}
### Customized options for this document
# Add necessary packages here
Packages <- c("tidyverse", "spatstat", "dbmss", "pbapply", "plyr", "GoFKernel", "microbenchmark")
# Install them
InstallPackages(Packages)

# knitr options
knitr::opts_chunk$set(
  cache =   TRUE,     # Cache chunk results
  include = TRUE,     # Show/Hide chunks
  echo =    TRUE,     # Show/Hide code
  warning = FALSE,    # Show/Hide warnings
  message = FALSE,    # Show/Hide messages
  # Figure alignment and size
  fig.align = 'center', out.width = '80%',
  # Graphic devices (ragg_png is better than standard png)
  dev = c("ragg_png", "pdf"),
  # Code chunk format
  tidy = FALSE, tidy.opts = list(blank=FALSE, width.cutoff=50),
  size = "scriptsize", knitr.graphics.auto_pdf = TRUE
  )
options(width = 50)

# ggplot style
library("tidyverse")
theme_set(theme_bw())
theme_update(panel.background=element_rect(fill="transparent", colour=NA),
             plot.background=element_rect(fill="transparent", colour=NA))
knitr::opts_chunk$set(dev.args=list(bg="transparent"))

# Random seed
set.seed(973)
```

# Motivation

Ce document montre comment utiliser le package *dbmss* [@Marcon2014] pour calculer la fonction $M$ [@Marcon2010] et teste l'impact de l'approximation du calcul proposée par @Tidu2023.

La première section fournit le code nécessaire à la création de données.
Des jeux de points de grande taille (de l'ordre de 100000 points) complètement aléatoires ou concentrés sont tirés.
L'approximation de leur position consiste à les rassembler au centre des cases d'une grille, selon l'approche de @Tidu2023 qui les positionnent au centre des unités administratives dans lesquelles ils se trouvent.

La deuxième section détaille l'utilisation de *dbmss* pour calculer la fonction $M$ et son intervalle de confiance à partir d'un tableau donnant la position et les caractéristiques des points ou bien une matrice de distance entre eux.

La troisième section teste l'impact de l'approximation des points.

Enfin, la quatrième section mesure la performance de *dbmss* en fonction de la taille du jeu de points.


# Données

## Tirage des points

Un jeu de points est tiré par un processus binomial dans une fenêtre carrée de côté 1.
La majorité des points constitue les "contrôles" et une partie constitue les "cas", dont la structure spatiale est étudiée.
Le poids des points est tiré dans une [loi gamma](https://fr.wikipedia.org/wiki/Loi_Gamma) dont les paramètre de forme et d’échelle sont libres.


Les paramètres sont:

- le nombre de points,
- la proportion de contrôles,
- la forme et l'échelle de la loi gamma.

```{r}
#| label: Parametres
points_nb <- 40000
case_ratio <- 1/20
size_gamma_shape <- 0.95
size_gamma_scale  <- 10
```


La fonction `X_csr()` permet de tirer un semis de points selon les paramètres.
L'argument `points_nb` qui fixe le nombre de points peut être modifé; les autres paramètres ont leur valeur fixée plus haut.

```{r}
library("tidyverse")
library("spatstat")
library("dbmss")
X_csr <- function(points_nb) {
  points_nb %>% 
    runifpoint() %>% 
    as.wmppp() ->
    X
  cases_nb <- round(points_nb *  case_ratio)
  controls_nb <- points_nb - cases_nb
  c(rep("Control", controls_nb), rep("Case", cases_nb)) %>% 
    as.factor() ->
    X$marks$PointType
  X$marks$PointWeight <- ceiling(
    rgamma(
      points_nb, 
      shape = size_gamma_shape, 
      scale = size_gamma_scale
    )
  )
  X
}

# Example
X <- X_csr(points_nb)
# Map the cases
autoplot(X[X$marks$PointType == "Case"])
# Point size distribution
hist(
  X$marks$PointWeight, 
  breaks = unique(X$marks$PointWeight), 
  main = "",
  xlab = "Point size")
```

La fonction `X_matern()` permet de tirer un semis de points dont les cas sont concentrés par un processus de Matérn.
Les paramètres sont:

- $\kappa$: le nombre d'aggrégats attendu,
- `scale`: leur rayon.

```{r}
# Expected number of clusters
kappa <- 20
# Cluster radius
scale <-  0.1
```

Le code de la fonction est le suivant:

```{r}
X_matern <- function(points_nb) {
  cases_nb <- round(points_nb *  case_ratio)
  controls_nb <- points_nb - cases_nb
  # CSR controls
  controls_nb %>% 
    runifpoint() %>% 
    superimpose(
      # Matern cases
      rMatClust(
        kappa = kappa, 
        scale = scale, 
        mu = cases_nb / kappa
      ) 
    ) %>% 
    as.wmppp() ->
    X
  # Update the number of cases
  cases_nb <- X$n - controls_nb
  c(rep("Control", controls_nb), rep("Case", cases_nb)) %>% 
    as.factor() ->
    X$marks$PointType
  X$marks$PointWeight <- ceiling(
    rgamma(
      X$n, 
      shape = size_gamma_shape, 
      scale = size_gamma_scale
    )
  )
  X
}

# Example
X <- X_matern(points_nb)
# Map the cases
autoplot(X[X$marks$PointType == "Case"])
```


## Grille

La fenêtre est découpée en une grille dont le nombre de cellules est `partitions` au carré.

```{r}
partitions <- 20
```

La fonction `group_points()` rassemble au centre de chaque cellule de la grille tous les points qu'elle contient pour mimer l'approximation habituelle de la position des points d'une unité administrative par la position de son centre.

```{r}
# Group points into cells
group_points <- function(X, partitions) {
X %>%
  with(tibble(
    x, 
    y, 
    PointType = marks$PointType, 
    PointWeight = marks$PointWeight)
  ) %>% 
  mutate(
    x_cell = ceiling(x * partitions) / partitions - 1 / 2 / partitions,
    y_cell = ceiling(y * partitions) / partitions - 1 / 2 / partitions
  ) %>%
  group_by(PointType, x_cell, y_cell) %>% 
  summarise(n = n(), PointWeight = sum(PointWeight)) %>% 
  rename(x = x_cell, y = y_cell) %>% 
  as.wmppp(window = X$window, unitname = X$window$units)
}
```

La position approximative est présentée sur la carte suivante.
Chaque cellule ne contient plus qu'un seul point de chaque type dont le poids est la somme de ceux des points individuels.

```{r}
group_points(X, partitions) %>% autoplot(alpha = 0.5)
```


# Calcul de M

Les distances auxquelles la fonction *M* est calculées sont choisies dans `r`.

```{r}
r <- c((0:9) / 100, (2:10) / 20)
```

## Données nécessaires

Dans le package *dbmss*, la fonction s'applique à un jeu de points, objet de classe `wmppp`, ou à une matrice de distance, objet de classe `Dtable`.

Nous partons d'un tableau (data.frame) contenant les colonnes `x`, `y`, `PointType` et `PointWeight`.

```{r}
X %>% 
  # Reduce to 10000 points
  rthin(P = 1E4 / points_nb) %>% 
  with(data.frame(x, y, marks)) ->
  points_df
head(points_df)
```

## Jeu de points

La fonction `wmppp()` permet de créer le jeu de points à partir du tableau.
La fenêtre est créée automatiquement si elle n'est pas précisée.
Ici, c'est un carré de côté 1.

```{r}
library("dbmss")
X <- wmppp(points_df, window = square(1))
```

La fonction `Mhat()` permet d'estimer la fonction.

```{r}
X %>% 
  Mhat(r = r, ReferenceType = "Case", NeighborType = "Control") %>% 
  autoplot()
```

La fonction `Menvelope()` permet de calculer l'intervalle de confiance de la valeur de la fonction sous l'hypothèse nulle de localisation aléatoire des points.

```{r}
X %>% 
  MEnvelope(r = r, ReferenceType = "Case", NeighborType = "Control") %>% 
  autoplot()
```

## Matrice de distances

La fonction `as.Dtable()` permet de créer un objet `Dtable`.

```{r}
d_matrix <- as.Dtable(points_df)
```

Il peut aussi être crée à partir d'une matrice de distances obtenue autrement, contenant par exemple des distances non euclidiennes (temps de transport, distance routière...).

```{r}
# A Dtable containing two points
Dmatrix <- matrix(c(0, 1, 1, 0), nrow = 2)
PointType <- c("Type1", "Type2")
PointWeight <- c(2, 3)
Dtable(Dmatrix, PointType, PointWeight)
```

Les fonctions `Mhat()` et `MEnvelope()` sont les mêmes que pour les jeux de points.

```{r}
identical(
  Mhat(X, r = r, ReferenceType = "Case", NeighborType = "Control"),
  Mhat(d_matrix, r = r, ReferenceType = "Case", NeighborType = "Control")
)
```


```{r}
d_matrix %>% 
  MEnvelope(r = r, ReferenceType = "Case", NeighborType = "Control") %>% 
  autoplot()
```

## Performance

Les deux traitements nécessitent un temps de calcul proche, légèrement inférieur pour le jeu de points.
Le calcul des distances est extrêmement rapide dans la fonction `Mhat()`: la matrice le fait économiser, mais le traitement complet est finalement plus long.

```{r}
library("microbenchmark")
(
  mb <- microbenchmark(
    Mhat(X, r = r, ReferenceType = "Case", NeighborType = "Control"),
    Mhat(d_matrix, r = r, ReferenceType = "Case", NeighborType = "Control"),
    times = 4L
  )
)
```


# Tests

La fonction `X_to_M()` calcule la fonction $M$ et renvoie le vecteur de ses valeurs pour chaque distance.
Elle est utile pour mesurer les temps d'exécution.

```{r}
# Compute M
X_to_M <- function(X) {
  X %>% 
  Mhat(r = r, ReferenceType = "Case", NeighborType  = "Control") %>% 
  pull("M")
}
```

## CSR

Le nombre de répétition des tests est fixé par `simulations_n`.

```{r}
simulations_n <- 10
```

`X_csr_list` contient `simulations_n` tirages du jeu de points.

```{r}
# Simulate X
X_csr_list <- replicate(simulations_n, X_csr(points_nb), simplify = FALSE)
```

Pour évaluer l'effet de l'approximation de la position, le calcul exact et le calcul sur les points de la grille sont effectués sur chaque jeu de points.

```{r}
library("pbapply")
# Compute M
system.time(M_csr_original <- pbsapply(X_csr_list, X_to_M))

# Group points and compute M
X_csr_grouped_list <- lapply(X_csr_list, group_points, partitions = partitions)
# Compute M
system.time(M_csr_grouped <- sapply(X_csr_grouped_list, X_to_M))
```

Le calcul approximé est très rapide parce qu'il réduit le nombre de points au double du nombre de cellules.

La corrélation entre les valeurs de $M$ calculées par chaque méthode est calculée à chaque distance.

```{r}
# Correlation
M_cor <- function(r_value, M_original, M_grouped) {
  r_index <- which(r == r_value)
  c(
    r_value,
    cor(M_original[r_index, ], M_grouped[r_index, ])
  ) 
}
sapply(r, M_cor, M_original = M_csr_original, M_grouped = M_csr_grouped) %>%
  t() %>% 
  as_tibble() %>% 
  rename(r = V1, correlation = V2) %>% 
  ggplot(aes(x = r, y = correlation)) +
    geom_point() +
    geom_line()
```

Les valeurs sont comparées.

```{r}
# Compare values
M_bias <- function(r_value, M_original, M_grouped) {
  r_index <- which(r == r_value)
    c(
    r_value,
    mean((M_grouped[r_index, ] - M_original[r_index, ]) / M_original[r_index, ]),
    sd(M_grouped[r_index, ] - M_original[r_index, ]) / mean(M_grouped[r_index, ]), 
    sd(M_original[r_index, ] / mean(M_original[r_index, ]))
  )
}
sapply(r, M_bias, M_original = M_csr_original, M_grouped = M_csr_grouped) %>% 
  t() %>% 
  as_tibble() %>% 
  rename(r = V1, `Relative error` = V2, `Error CV` = V3, `M CV` = V4) %>% 
  ggplot() +
    geom_point(aes(x = r, y = `Relative error`)) +
    geom_errorbar(
      aes(
        x = r, 
        ymin = `Relative error` - `Error CV`, 
        ymax = `Relative error` + `Error CV`
      )
    ) +
    geom_errorbar(aes(x = r, ymin = 0, ymax = `M CV`), col = "red")
```

La valeur moyenne de $M$ est 1 à toutes les distances par construction: les cas et les contrôles sont complètement aléatoires.

Les barres rouges représentent l'écart-type empirique de la valeur de $M$, calculé à partir des simulations.
Les points noirs montrent l'erreur apportée par l'approximation, mesurée par l'écart moyen entre les valeurs de $M$ calculées avec ou sans approximation.
Les barres d'erreur sont l'écart-type de cette différence.

L'approximation surestime systématiquement $M$, au point de détecter une concentration spatiale qui n'existe pas.
L'erreur est importante jusqu'à la taille de la grille: tous les points d'une même cellule sont concentrés artificiellement en son centre.
Elle chute brutalement au-delà de ce seuil mais reste importante jusqu'à 5 ou 6 fois la taille de la grille.

L'approximation de doit pas être utilisée pour étudier les interactions à courte distance.

Le test effectué ici est beaucoup plus sévère que dans l'article: les points n'ont aucune structure, donc $M$ permet de détecter les petites variations aléatoires des différents tirages.
En présence d'une structure spatiale, les valeurs de $M$ sont nettement mieux corrélées.


## Matérn

```{r}
# Simulate X
X_matern_list <- replicate(simulations_n, X_matern(points_nb), simplify = FALSE)
```

Pour évaluer l'effet de l'approximation de la position, le calcul exact et le calcul sur les points de la grille sont effectués sur chaque jeu de points.

```{r}
# Compute M
system.time(M_matern_original <- pbsapply(X_matern_list, X_to_M))

# Group points and compute M
X_matern_grouped_list <- lapply(X_matern_list, group_points, partitions = partitions)
# Compute M
system.time(M_matern_grouped <- sapply(X_matern_grouped_list, X_to_M))
```

La corrélation entre les valeurs de $M$ calculées par chaque méthode est calculée à chaque distance.

```{r}
# Correlation
sapply(r, M_cor, M_original = M_matern_original, M_grouped = M_matern_grouped) %>%
  t() %>% 
  as_tibble() %>% 
  rename(r = V1, correlation = V2) %>% 
  ggplot(aes(x = r, y = correlation)) +
    geom_point() +
    geom_line()
```

Les valeurs sont comparées.

```{r}
# Compare values
sapply(r, M_bias, M_original = M_matern_original, M_grouped = M_matern_grouped) %>% 
  t() %>% 
  as_tibble() %>% 
  rename(r = V1, `Relative error` = V2, `Error CV` = V3, `M CV` = V4) %>% 
  ggplot() +
    geom_point(aes(x = r, y = `Relative error`)) +
    geom_errorbar(
      aes(
        x = r, 
        ymin = `Relative error` - `Error CV`, 
        ymax = `Relative error` + `Error CV`
      )
    ) +
    geom_errorbar(aes(x = r, ymin = 0, ymax = `M CV`), col = "red")
```

La corrélation est très élevée dès que la distance prise en compte dépasse le double de la maille de la grille et les erreurs sont assez faibles.


# Performance de M

## Temps de calcul

Le temps de calcul nécessaire au calcul exact est évalué pour une gamme de nombres de points précisée dans `X_sizes`.

```{r}
X_sizes <- c(1000, 5000, 10000, 50000, 100000)
```

La fonction `test_time()` permet de mesurer le temps d'exécution d'une évaluation de la fonction $M$.

```{r}
library("microbenchmark")
test_time <-function(points_nb) {
  X <- X_csr(points_nb)
  microbenchmark(X_to_M(X), times = 4L) %>% 
    pull("time")
}

X_sizes %>% 
  sapply(test_time) %>% 
  as_tibble() %>% 
  pivot_longer(cols = everything()) %>% 
  rename(Size = name) %>% 
  group_by(Size) %>% 
  summarise(Time = mean(value) / 1E9, sd = sd(value) / 1E9) %>% 
  mutate(
    Size = as.double(
      plyr::mapvalues(
        .$Size, 
        from = paste0("V",seq_along(X_sizes)), 
        to = X_sizes
      )
    )
  ) -> M_time
M_time %>% 
  ggplot(aes(x = Size, y = Time)) +
    geom_point() +
    geom_errorbar(aes(ymin = Time - sd, ymax = Time + sd)) +
    scale_x_log10() +
    scale_y_log10()
```

Le temps de calcul est lié à la taille du jeu de points par une loi puissance.

```{r}
# Model
M_time %>% 
  mutate(logTime = log(Time), logSize = log(Size)) ->
  M_time_log
M_time_lm <- lm(logTime ~ logSize, data = M_time_log) 
summary(M_time_lm)
```

Le temps de calcul augmente moins vite que le carré du nombre de points.
Il peut être estimé très précisément ($R^2=$ `r format(summary(M_time_lm)$r.squared, digits=2)`) par la relation $t = t_0 (n / n_o)^p$ où $t$ est le temps estimé pour $n$ points connaissant le temps $t_0$ (ex.: `r as.integer(M_time[5, 1])`) pour $n_0$ points (ex.: `r format(as.numeric(M_time[4, 2]), digits=3)`) et $p$ la relation de puissance (`r format(M_time_lm$coefficients[2], digits=3)`).


## Mémoire

La mémoire utilisée est évaluée pour les mêmes tailles de données.

```{r}
# RAM
library("profmem")
test_ram <-function(points_nb) {
  X <- X_csr(points_nb)
  profmem(X_to_M(X)) %>% 
    pull("bytes") %>% 
    sum()
}
sapply(X_sizes, test_ram) %>% 
  tibble(Size = X_sizes, RAM = . / 2^20) ->
  M_ram
M_ram %>% 
  ggplot(aes(x = Size, y = RAM)) +
    geom_point() +
    geom_line()
```

La mémoire nécessaire (en Mo) augmente linéairement avec le nombre de points et n'est jamais critique pour des tailles de jeux de points traitables dans des temps raisonnables.

```{r}
# Model
lm(RAM ~ Size, data = M_ram) %>% summary()
```

La mémoire utilisée par les objets `Dtable` pour le calcul de $M$ à partir d'une matrice de distance est bien supérieure: c'est celle d'une matrice numérique, de l'ordre de 8 octets fois le nombre de points au carré, soit 800 Mo pour 10000 points seulement.


# Conclusion

Le temps de calcul de $M$ est de l'ordre de 6 secondes pour un jeu de 100 000 points sur un ordinateur portable (processeur Intel i7-1360P 2.20 GHz), et nécessite 25 Mo de RAM.
Le calcul d'un intervalle de confiance à partir de 1000 simulations prend donc moins de deux heures.

Pour un jeu de cinq millions de points, le temps de calcul attendu est $6 \times 50^{1.8} = 6860$ secondes, près de deux heures.
1000 simulations nécessiteraient alors environ trois mois.

Le calcul des distances est parallélisé: un serveur de calcul augmente drastiquement la performance.

Le calcul exact se justifie donc pleinement pour des données de l'ordre de $10^5$ points: quelques heures suffisent à caculer des intervalles de confiance.

Au-delà, l'approximation de la localisation permet de ramener la taille du jeu de points à celle du nombre de localisations retenues.
Le prix à payer est l'absence d'information à l'échelle des unités géographiques élémentaires (les cellules de la grille ici), et une précision dégradée jusqu'à environ 5 fois cette échelle dans le pire des cas, c'est-à-dire l'absence de structure spatiale.


`r if (!knitr:::is_latex_output()) '# References {-}'`
